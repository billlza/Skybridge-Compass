#!/usr/bin/env python3
"""
Aggregate real-network artifacts into a paper-friendly summary and a LaTeX table.

Inputs (generated by scripts):
  - Artifacts/realnet_stun_summary_<stamp>_<label>.csv
  - Artifacts/realnet_e2e_summary_<stamp>_<label>.csv  (one row per payload size)

Output:
  - Artifacts/realnet_microstudy_<ARTIFACT_DATE>.csv
  - Docs/supp_tables/s9_realnet_microstudy.tex

Notes:
  - This script is conservative: it only merges rows where STUN summary exists and at least
    one E2E summary row exists for the same <stamp> and <label>.
  - For paper-style comparisons, we prefer two payload sizes:
      classic_bytes=687, pqc_bytes=12002
  - Use ARTIFACT_DATE=YYYY-MM-DD to keep output stable for paper builds.
"""

from __future__ import annotations

import csv
import os
from dataclasses import dataclass
from pathlib import Path


ROOT = Path(__file__).resolve().parent.parent
ARTIFACTS = ROOT / "Artifacts"
DOCS_SUPP = ROOT / "Docs" / "supp_tables"


def _env_artifact_date() -> str | None:
    v = os.environ.get("ARTIFACT_DATE") or os.environ.get("SKYBRIDGE_ARTIFACT_DATE")
    return v.strip() if v and v.strip() else None


def _find_pairs() -> list[tuple[Path, Path, str, str]]:
    """Return list of (stun_summary, e2e_summary, stamp, label) pairs."""
    stuns = {}
    for p in ARTIFACTS.glob("realnet_stun_summary_*_*.csv"):
        # pattern: realnet_stun_summary_<stamp>_<label>.csv
        name = p.name
        rest = name[len("realnet_stun_summary_"):-4]
        if "_" not in rest:
            continue
        stamp, label = rest.split("_", 1)
        stuns[(stamp, label)] = p

    pairs = []
    for p in ARTIFACTS.glob("realnet_e2e_summary_*_*.csv"):
        name = p.name
        rest = name[len("realnet_e2e_summary_"):-4]
        if "_" not in rest:
            continue
        stamp, label = rest.split("_", 1)
        sp = stuns.get((stamp, label))
        if sp:
            pairs.append((sp, p, stamp, label))
    return sorted(pairs, key=lambda t: (t[2], t[3]))


def _read_single_row_csv(path: Path) -> dict[str, str]:
    with path.open("r", newline="") as f:
        r = csv.DictReader(f)
        rows = list(r)
    if not rows:
        raise ValueError(f"{path} has no rows")
    # If multiple rows exist, take the first (the scripts write 1 row).
    return rows[0]


def _read_rows_csv(path: Path) -> list[dict[str, str]]:
    with path.open("r", newline="") as f:
        r = csv.DictReader(f)
        return list(r)


@dataclass(frozen=True)
class MicroRow:
    stamp: str
    label: str
    path_status: str
    interface_types: str
    expensive: str
    constrained: str
    nat: str
    stun_loss: str
    stun_rtt_p50: str
    stun_rtt_p95: str
    classic_ok_rate: str
    classic_total_p50: str
    pqc_ok_rate: str
    pqc_total_p50: str
    delta_total_p50: str
    pqc_timeout_rate: str
    pqc_other_fail_rate: str


def _fmt(x: str | None) -> str:
    return (x or "").strip()


def main() -> None:
    pairs = _find_pairs()
    if not pairs:
        print("No paired realnet summaries found (need both STUN + E2E).")
        return

    out_date = _env_artifact_date() or pairs[-1][2]  # prefer pinned, else latest stamp
    rows: list[MicroRow] = []

    classic_bytes = 687
    pqc_bytes = 12002

    for stun_path, e2e_path, stamp, label in pairs:
        stun = _read_single_row_csv(stun_path)
        e2e_rows = _read_rows_csv(e2e_path)

        by_bytes: dict[int, dict[str, str]] = {}
        for r in e2e_rows:
            try:
                b = int((r.get("payload_bytes") or "").strip())
            except ValueError:
                continue
            by_bytes[b] = r

        def get(payload: int, key: str) -> str:
            return _fmt(by_bytes.get(payload, {}).get(key))

        # Prefer p50 total time as a single "user-visible" metric.
        classic_ok = get(classic_bytes, "ok_rate")
        classic_total_p50 = get(classic_bytes, "total_p50_ms")
        pqc_ok = get(pqc_bytes, "ok_rate")
        pqc_total_p50 = get(pqc_bytes, "total_p50_ms")

        # Failure taxonomy (PQC side)
        pqc_timeout_rate = get(pqc_bytes, "timeout_rate")
        # other fail rate = 1 - ok_rate - timeout_rate (if present)
        pqc_other_fail_rate = ""
        try:
            if pqc_ok and pqc_timeout_rate:
                other = max(0.0, 1.0 - float(pqc_ok) - float(pqc_timeout_rate))
                pqc_other_fail_rate = f"{other:.4f}"
        except ValueError:
            pass

        delta_total_p50 = ""
        try:
            if classic_total_p50 and pqc_total_p50:
                delta = float(pqc_total_p50) - float(classic_total_p50)
                delta_total_p50 = f"{delta:.3f}"
        except ValueError:
            pass

        rows.append(MicroRow(
            stamp=stamp,
            label=label,
            path_status=_fmt(stun.get("path_status")),
            interface_types=_fmt(stun.get("path_interface_types")),
            expensive=_fmt(stun.get("path_is_expensive")),
            constrained=_fmt(stun.get("path_is_constrained")),
            nat=_fmt(stun.get("nat_classification")),
            stun_loss=_fmt(stun.get("loss_rate")),
            stun_rtt_p50=_fmt(stun.get("rtt_p50_ms")),
            stun_rtt_p95=_fmt(stun.get("rtt_p95_ms")),
            classic_ok_rate=classic_ok,
            classic_total_p50=classic_total_p50,
            pqc_ok_rate=pqc_ok,
            pqc_total_p50=pqc_total_p50,
            delta_total_p50=delta_total_p50,
            pqc_timeout_rate=pqc_timeout_rate,
            pqc_other_fail_rate=pqc_other_fail_rate,
        ))

    # Write merged CSV (paper artifact)
    csv_out = ARTIFACTS / f"realnet_microstudy_{out_date}.csv"
    with csv_out.open("w", newline="") as f:
        w = csv.writer(f)
        w.writerow([
            "stamp", "label",
            "path_status", "interface_types", "is_expensive", "is_constrained",
            "nat_class", "stun_loss_rate", "stun_rtt_p50_ms", "stun_rtt_p95_ms",
            "classic_ok_rate", "classic_total_p50_ms",
            "pqc_ok_rate", "pqc_total_p50_ms",
            "delta_total_p50_ms",
            "pqc_timeout_rate", "pqc_other_fail_rate",
        ])
        for r in rows:
            w.writerow([
                r.stamp, r.label,
                r.path_status, r.interface_types, r.expensive, r.constrained,
                r.nat, r.stun_loss, r.stun_rtt_p50, r.stun_rtt_p95,
                r.classic_ok_rate, r.classic_total_p50,
                r.pqc_ok_rate, r.pqc_total_p50,
                r.delta_total_p50,
                r.pqc_timeout_rate, r.pqc_other_fail_rate,
            ])

    # Write LaTeX table for supplementary
    DOCS_SUPP.mkdir(parents=True, exist_ok=True)
    tex_out = DOCS_SUPP / "s9_realnet_microstudy.tex"

    def esc(s: str) -> str:
        return (s.replace("_", r"\_")
                .replace("%", r"\%"))

    lines = []
    lines.append("% Auto-generated by aggregate_realnet.py")
    lines.append(r"\begin{table*}[!t]")
    lines.append(r"\centering")
    lines.append(r"\scriptsize")
    lines.append(r"\caption{Supplementary Table \thetable: Real-network micro-study (STUN path + TCP payload). Each row is one network condition label. STUN metrics capture path RTT/loss and a conservative NAT classification. E2E metrics report success rate and p50 completion time for two payload sizes (classic 687~B vs PQC 12{,}002~B), along with the delta (PQC minus classic). Failure taxonomy is summarized for the PQC size as timeout rate and other failure rate.}")
    lines.append(r"\label{tab:supp-realnet-microstudy}")
    # Fit wide table within page bounds.
    lines.append(r"\setlength{\tabcolsep}{3pt}")
    lines.append(r"\resizebox{\textwidth}{!}{%")
    # 13 columns: 2 text (Label, IFace) + 11 numeric/status fields
    lines.append(r"\begin{tabular}{@{}llccccccccccc@{}}")
    lines.append(r"\toprule")
    lines.append(r"Label & IFace & Exp & Con & NAT & STUN loss & STUN RTT p50/p95 (ms) & ok$_c$ & p50$_c$ (ms) & ok$_p$ & p50$_p$ (ms) & $\Delta$p50 (ms) & PQC fail (to/other) \\")
    lines.append(r"\midrule")
    for r in rows:
        iface = esc(r.interface_types or "-")
        exp = esc(r.expensive or "-")
        con = esc(r.constrained or "-")
        nat = esc(r.nat or "-")
        stun_loss = esc(r.stun_loss or "-")
        rtt = f"{esc(r.stun_rtt_p50 or '-')}/{esc(r.stun_rtt_p95 or '-')}"
        ok_c = esc(r.classic_ok_rate or "-")
        p50_c = esc(r.classic_total_p50 or "-")
        ok_p = esc(r.pqc_ok_rate or "-")
        p50_p = esc(r.pqc_total_p50 or "-")
        d50 = esc(r.delta_total_p50 or "-")
        fail = f"{esc(r.pqc_timeout_rate or '-')}/{esc(r.pqc_other_fail_rate or '-')}"
        lines.append(
            f"{esc(r.label)} & {iface} & {exp} & {con} & {nat} & {stun_loss} & {rtt} & {ok_c} & {p50_c} & {ok_p} & {p50_p} & {d50} & {fail} \\\\"
        )
    lines.append(r"\bottomrule")
    lines.append(r"\end{tabular}")
    lines.append(r"}")
    lines.append(r"\end{table*}")

    tex_out.write_text("\n".join(lines) + "\n", encoding="utf-8")

    print(f"Wrote: {csv_out}")
    print(f"Wrote: {tex_out}")


if __name__ == "__main__":
    main()


